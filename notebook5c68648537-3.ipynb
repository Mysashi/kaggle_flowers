{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Импорты библиотек","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport cv2\nimport os\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nfrom os import listdir\n\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Определение основных переменных ","metadata":{}},{"cell_type":"code","source":"MAX_PIXELS_WIDTH = 126 # ширина картинки\nMAX_PIXELS_HEIGHT = 126 # высота картинки\nBATCH_SIZE = 20 # размер батча\nX = [] # представление изображения в виде массива\nZ = [] # лэйблы изображений\n\n# переменные для обработки изображений\nINITIAL_FLOWER_DIR = \"../input/flowers-recognition/flowers\"\nFLOWER_DAISY_DIR='../input/flowers-recognition/flowers/daisy'\nFLOWER_SUNFLOWER_DIR='../input/flowers-recognition/flowers/sunflower'\nFLOWER_TULIP_DIR='../input/flowers-recognition/flowers/tulip'\nFLOWER_DANDI_DIR='../input/flowers-recognition/flowers/dandelion'\nFLOWER_ROSE_DIR='../input/flowers-recognition/flowers/rose'\n\n\n\nflowers_dirs = [FLOWER_DAISY_DIR, FLOWER_SUNFLOWER_DIR, FLOWER_TULIP_DIR, FLOWER_DANDI_DIR, FLOWER_ROSE_DIR]\nflowers_label = [\"daisy\", \"sunflower\",\"tulip\", \"dandelion\", \"rose\"]\n\nprint()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=flower_type\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (MAX_PIXELS_WIDTH,MAX_PIXELS_HEIGHT))\n       \n        \n        X.append(np.array(img))\n        Z.append(str(label))\n\n\nfor i in range(5):\n    make_train_data(flowers_label[i], flowers_dirs[i])\n\nprint(F\"Length of X array len(X)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nle=LabelEncoder()\nY=le.fit_transform(Z)\nY=to_categorical(Y,5)\nX=np.array(X)\nX=X/255\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (MAX_PIXELS_WIDTH,MAX_PIXELS_HEIGHT,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\n# model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = Adam(learning_rate=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(x_train,y_train,epochs=20,batch_size=BATCH_SIZE,validation_data = (x_test,y_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Замчен underfitting\n","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"History = model.fit(datagen.flow(x_train,y_train, batch_size=BATCH_SIZE),\n                              epochs = 10, validation_data = (x_test,y_test),\n                             )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}